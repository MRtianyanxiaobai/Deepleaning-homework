# *_*coding:utf-8 *_*
import numpy as  np
import  matplotlib.pylab as plt
import sklearn
from sklearn import  datasets
#是设置相同的seed，随机出来的数相同
def load_planer_dataset():
        np.random.seed(1)
        m = 400 #设置样本数量
        N = int(400/2)
        D = 2
        X = np.zeros((m,D))
        Y = np.zeros((m,1))
        a = 4#定义花的最大长度

        for j in range(2):
            ix = range(N * j, N * (j + 1))
            t = np.linspace(j * 3.12, (j + 1) * 3.12, N) + np.random.randn(N) * 0.2  # theta
            r = a * np.sin(4 * t) + np.random.randn(N) * 0.2  # radius
            X[ix] = np.c_[r * np.sin(t), r * np.cos(t)]
            Y[ix] = j

        X = X.T
        Y = Y.T

        return X, Y

##数据集测试
# X,Y =load_planer_dataset()
# #np.squeeze(Y) Y是一个二维列表 通过该函数转换为一维
# plt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral) #绘制散点图
# plt.show()

##函数
def sigmoid(x):
    s = 1/(1+np.exp(-x))
    return s

def Relu(x):
    x = np.maximum(0,x)
    return x

##定义神经网络的输入层，输出层
def layer_size(X,Y):
    n_input = X.shape[0]
    n_h = 4
    n_output =Y.shape[0]
    return (n_input,n_h,n_output)

def init_paramters(n_input,n_h,n_output):
  #  np.random.seed(2) #指定一个随机种子，以便你的输出与我们的一样。
    w1 = np.random.randn(n_h,n_input)*0.01
    b1 = np.zeros((n_h,1))

    w2 = np.random.randn(n_output,n_h)*0.01
    b2 = np.zeros((n_output,1))

    parmaters={
        "w1":w1,
        "b1":b1,
        "w2":w2,
        "b2":b2
    }
    return parmaters

##测试
# n_input,n_h,n_output = layer_size(X,Y)
# params = init_paramters(n_input,n_h,n_output)
# print("w1",params['w1'].shape)
# print("b1",params['b1'].shape)
# print("w2",params['w2'].shape)
# print("b2",params['b2'].shape)

#前项传播
def forward_propagation(X,params):
    w1 = params['w1']
    w2  =params['w2']
    b1 = params['b1']
    b2 = params['b2']

    Z1 = np.dot(w1,X)+b1
    A1 = np.tanh(Z1)

    Z2 = np.dot(w2,A1)+b2
    A2 = sigmoid(Z2)
    #将这些结果缓存下来供方向传播使用
    cache={
        "Z1":Z1,
        "A1":A1,
        "Z2":Z2,
        "A2":A2
    }
    return (A2,cache)

# A2,cache = forward_propagation(X,params)
# print(np.mean(cache["Z1"]), np.mean(cache["A1"]), np.mean(cache["Z2"]), np.mean(cache["A2"]))

#计算损失
def compute_cost(A2,Y,params):
    m = Y.shape[1]

    loss = np.multiply(Y,np.log(A2))+np.multiply(1-Y,np.log(1-A2))
    J = np.mean(loss)
    cost = float(np.squeeze(J))

    return -cost


##反向传播
def backward_propagation(parms,cache,X,Y):
    A1,A2,Z1,Z2 = cache['A1'],cache['A2'],cache['Z1'],cache['Z2']

    m = Y.shape[1]
    dz2 = A2-Y
    dw2 = np.dot(dz2,A1.T)/m
    #keepdim保持维度
    db2 = (1/m) * np.sum(dz2,axis=1,keepdims=True)

    w2 = parms['w2']
    #这里tanh求导为1-A的平方
    dz1 = np.multiply(np.dot(w2.T,dz2), 1- np.power(A1,2))

    dw1 = (1/m)* np.dot(dz1,X.T)
    db1 = (1/m)*np.sum(dz1,axis=1,keepdims=True)

    grads={
        "dw1":dw1,
        "dw2":dw2,
        "db1":db1,
        "db2":db2
    }
    return  grads

#测试
# grads = backward_propagation(params,cache,X,Y)
# print("dw1="+str(grads['dw1']))
# print("dw2="+str(grads['dw2']))
# print("db1="+str(grads['db1']))
# print("db2="+str(grads['db2']))


#利用梯度下降更新参数
def updata_params(params,grads,learning_rate=1.2):
    w1,w2 = params['w1'],params['w2']
    b1,b2 = params['b1'],params['b2']

    dw1,dw2 = grads['dw1'],grads['dw2']
    db1,db2 =grads['db1'],grads['db2']

    w1 = w1-learning_rate*dw1
    w2 = w2-learning_rate*dw2
    b1 = b1-learning_rate*db1
    b2 = b2-learning_rate*db2
    parmaters = {
        "w1": w1,
        "b1": b1,
        "w2": w2,
        "b2": b2
    }
    return parmaters


#将上述函数进行整合，建立完整的神经网络
def nn_modle(X,Y,n_h,num_iterations):
    # X:训练数据
    # Y:标签
    # n_h:隐藏层节点数

    np.random.seed(3) #指定随机种子
    n_input,n_h,n_output = layer_size(X,Y)
    params = init_paramters(n_input,n_h,n_output)
    w1,w2 = params['w1'],params['w2']
    b1,b2 = params['b1'],params['b2']
    for i in range(num_iterations):
        A2, cache = forward_propagation(X, params)

        grads = backward_propagation(params, cache, X, Y)
        params=updata_params(params, grads)

        cost = compute_cost(A2, Y, params)

        if(i%10000==0):
            print("第",i,"次循环，成本为:"+str(cost))
    return params

#编写预测函数
def predit(X,params):
    A2, cache = forward_propagation(X, params)
    #二分类，sigmod小于0.5则为0
    predictions = np.round(A2)
    return predictions

#画边界
def plot_decision_boundary(model, X, y):
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    plt.scatter(X[0, :], X[1, :], c=np.squeeze(y), cmap=plt.cm.Spectral)


#最终测试
X,Y =load_planer_dataset()
params = nn_modle(X, Y, 4, 100000)
yhat = predit(X,params)
socre = np.mean(np.array(yhat==Y,dtype=np.int))
